# -*- coding: utf-8 -*-
"""BT1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p_lXV0aAb6Bb6bEZcHtr17LuV8B64M44
"""

!pip install pyspark

from pyspark  import SparkConf, SparkContext
import collections

conf = SparkConf().setMaster('local').setAppName('Word counting')
sc = SparkContext.getOrCreate(conf = conf)
myRDD = (
    sc
    .textFile(
        '/content/sample_data/Text.txt').map(lambda each: each.split()))
counts = myRDD.reduce(lambda x:1)
words = len(counts)
print(words)

myrdd = sc.parallelize(counts)
temp = myrdd.map(lambda x:(x,1))
number_of_word = temp.reduceByKey(lambda x,y: x+y)
#print(number_of_word.collect())
k=int(input('Enter k: '))
rls = number_of_word.takeOrdered(k, key = lambda x: -x[1])
for i in rls:
  print(i[0])
#max_i=number_of_word.reduce(lambda x,y: max(x,y))
#print(max_i)
